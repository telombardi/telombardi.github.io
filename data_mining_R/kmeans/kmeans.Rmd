---
title: 'K-means Clustering'
output: 
    tufte::tufte_html:
      highlight: tango
---

#K-Means Clustering

In our last class, we learned hierarchical clustering, a bottom-up approach to grouping observations. Today, we'll start talking about K-means clustering. 

The K in K-means stands for the number of clusters we expect in our data. In other words, the K-means algorithm expects us to tell it how many clusters to find. In some cases, we will have a good idea about how to cluster the data because we 
understand the data set well. For example, the iris data contains three species and therefore k=3 is a natural choice. In other cases, we will need to experiment to find decent numbers for k. The whole purpose of clustering is to find natural groups in our data.

Since the K-means algorithm depends on the repeated calculation of the arithmetic mean, this approach to clustering is susceptible to the same kinds of problems we observe with the arithmetic mean. K-means assumes roughly normally distributed variables; outliers will skew the results. We can handle this problem by removing unusual observations, transforming variables or using techniques less sensitive to outliers like K-medians clustering. 

##Data Preparation and Cleansing

###Load Data

```{r}
data <- read.csv("http://cis241.washjeff-cis.net/iris_data.csv")
```

###Scale Data

Since our data are not necessarily at the same scale, we should standardize them.
In the code below, we copy the data to a new data frame, remove the Species field from the new data frame, and scale the data.

```{r}
sdata <- data
sdata$Species <- NULL
sdata <- scale(data[,1:4])
summary(sdata)
```

##K-means

We execute the K-means algorithm with the following code.
_sdata_ represents the data we mean to cluster. In this case, 
we are looking for 3 clusters because we have three types of irises.
_nstart_ specifies how many random sets of centroids should be chosen.
_iter.max_ defines the maximum number of iterations used to find centroids.

```{r}
clusters <- kmeans(sdata,3, nstart=100000, iter.max=100000)
clusters
```

_Cluster means_ gives us the centroid values per variable.

_Clustering vector_ provides the cluster assignment of each data point.

_Available components:_ lists other variables we can access for additional information.

A decent example of an additional available component is _totss_, the total sum of squares, which reports the total of the squared distances between each observation and each centroid.

#Evaluation

K-means does not produce a metric that we can simply report. We have to evaluate the clusters based on our domain knowledge. We often want to visualize our clusters to get a better sense of how the data form groups. Unfortunately, this isn't always easy with many variables.

Because we know so much about Anderson's Iris data, we can see how well our clusters conform to known classifications.

```{r}
table(data$Species,clusters$cluster)
```

This table should look somewhat familiar. The Iris setosa is fairly easy to identify and our clustering picks it up perfectly. We also see some confusion in the I. versicolor and I. virginica species.

In order to visualize our data, we need to reduce it to only two variables so that we can graph it. Luckily, linear algebra gives us a convenient way to do this: principal components (PCA). Basically, the principal components capture the variance across all variables. By selecting the first 2 principal components, we can graph our observations in two dimensions. You can think of principal components as metavariables that capture most of the variance in our data with fewer measurements.

Remember to install packages when necessary, but you should only do so in interactive or script mode.

```
install.packages("ggplot2")
```

```{r}
library(ggplot2)
princ <- prcomp(sdata)
project <- predict(princ, newdata=sdata)[,1:2]
project.plus <- cbind(as.data.frame(project), cluster=clusters$cluster, species=data$Species)
ggplot(project.plus, aes(x=PC1, y=PC2))+
       geom_point(aes(shape=as.factor(cluster)))+
       geom_text(aes(label=species), hjust=0, vjust=1)

```

We can also use the somewhat easier technique in the `cluster` package.

```
install.packages("cluster")
```

```{r}
library(cluster)
clusplot(sdata[,1:4], clusters$cluster, color=TRUE, shade=TRUE,
   labels=2, lines=0)
```

We can see that I. versicolor and I. virginica overlap significantly.
Are there variables that contain more noise than others?

```{r}
plot(sdata[,c("Sepal.length","Sepal.width")], col=clusters$cluster)
points(clusters$centers[,c("Sepal.length","Sepal.width")],col=1:3,pch=8,cex=2)

plot(sdata[,c("Petal.length","Petal.width")], col=clusters$cluster)
points(clusters$centers[,c("Petal.length","Petal.width")],col=1:3,pch=8,cex=2)

plot(sdata[,c("Petal.length","Sepal.length")], col=clusters$cluster)
points(clusters$centers[,c("Petal.length","Sepal.length")],col=1:3,pch=8,cex=2)

plot(sdata[,c("Petal.width","Sepal.width")], col=clusters$cluster)
points(clusters$centers[,c("Petal.width","Sepal.width")],col=1:3,pch=8,cex=2)
```

##Repeat with subset of data - remove noisy features

Reviewing the combinations of features above, we might try to cluster our data using only Petal.length and Petal.width as these seem to separate the samples more cleanly than other combinations of variables.

```{r}
clusters <- kmeans(sdata[,c("Petal.length","Petal.width")],3, nstart=100000, iter.max=100000)
clusters
table(data$Species,clusters$cluster)
```

We can see that our clustering can be improved by removing noisy variables.
Although the clusters are not perfectly formed, we have a fairly clean separation of known classes based only on Petal.length and Petal.wdith measurements.

##Outliers can be a problem
Since kmeans is based on the mean of each variable, outliers can throw off 
the results. Consider if you have any outliers that should be removed.

##How do we know if we have a decent K?

This is a great question with some reasonably good answers.
Since we have a powerful language like R at our disposal, we might 
simply run k-means many times with different k values. Then we can plot the within groups sum of squares against the number of clusters.

```{r}
wss <- (nrow(sdata)-1)*sum(apply(sdata,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(sdata,
                                       centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
```

We are looking for the elbow in the graph or the point at which adding more clusters does not significantly reduce the within groups sum of squares. In this case, we see the elbow occurring at roughly 3 or 4.

